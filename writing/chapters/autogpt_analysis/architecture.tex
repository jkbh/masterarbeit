\documentclass[../../main.tex]{subfiles}
\begin{document}
The AutoGPT agent is modeled after the classic agent architecture.
After the start,
the user is asked to enter a task the AutoGPT agent should perform.
Then the agent enters a loop of prompting the LLM,
executing the proposed action
and handling the result of the action and updating the agent state.

The LLM is prompted in a structured way.
A base prompt template is defined
and populated with current information before each prompting step.
The information includes the task at hand,
a list of possible actions,
a history of previous actions and their results
and some extra statements that are there to guide the language model.
As the answer needs to be parsed,
the system prompt defines a fixed format the LLM should answer in.
The answer consists of the thoughts and the proposed next action.

AutoGPT is divided into four modules.
The \textit{brain} is the main module that controls the agent.
In AutoGPT this is realized by prompting the language model in a structured way.
Using the chat system prompt,
the language model is prompted to answer a structred format.
Different techniques are implemented in this structure.
The language model is forced not only to plan the next step,
but also to explain the choice for the chosen step and to add self-criticism.
An extra output for the human user is also returned.
The second part of the answer is the actual next action with the needed arguments.
The action make up the second module of the agent.
In this module the abilities of the agent are defined.
These can be file operations,
database queries or web search functionalities.
The third module is the \textit{memory}.
Memories are modeled after humans which have short and long-term memory.
Short term memory can be implemented
as an in-memory list of messages to the language model.
Long-term memory needs a persistent store such as a database.
A popular option for language models are vector databases that work with embeddings.

Currently,
the AutoGPT agent is implemented for OpenAI GPT models.
The prompts are tailored in ways
that benefit the charactericstics of GPT-3 and GPT-4.
Switching to a different model is not diffult from a software perspective,
but semantically poses a challenge.
If one knows the message format for a specific model the input can be adjusted.
But the same prompting techniques does not necessarily work for all langauge models.
The challenge is to switch between different prompting styles,
as every model needs to be prompted differently.
For example,
OpenAI models benefit from profile sentences like "You are an expert in computer science",
while Anthropics Claude does not\dots
\end{document}