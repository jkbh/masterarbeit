\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{A Retrieval Augmented Generation Agent for IR}
\label{ch:agent}

Large language models are excellent at generating text in a variety of styles.
But when prompted about specific topics the answer quality suffers.
Different approaches try to tackle this problem.
A promising method is \gls{rag}, which combines in-context learning prompting techniques with a vector database.
In this section, I present an agent that is designed to answer user prompts in a retrieval-augmented fashion.

To create the \gls{ir} agent,
I used the Forge SDK \autocite{zotero-117} that is included in \gls{autogpt}.
The Forge agent SDK is a tool to create a custom agent without having to write the boilerplate code.
In comparison with the \gls{autogpt} agent, it comes with less predefined abilities and has no initial logic.
On the contrary, fewer parts can break, and it is not under an ongoing re-factoring process,
compared to the \gls{autogpt} agent.
Additionally, the \gls{autogpt} agent is not optimized for \gls{ir} over a set of documents
but operates more as a general-purpose agent.

I extended the Forge agent with abilities that are needed for \gls{rag}.
To store the documents for retrieval, I used a \emph{ChromaDB} vector database.
The 'ingest' ability of the agent can be used to ingest a PDF file that is saved in the workspace.
The PDF is converted into plain text and then chunked into smaller documents using the \gls{sentsplitter} from \gls{llamaindex} \cite{zotero-255},
which produces chunks of roughly equal length while respecting sentence boundaries.

\section{Methods To Improve LLM Generation}

Different approaches try to embed information sources into the generated text.
One approach is to fine-tune the language model on a dataset that contains the information.
The creation of fine-tuning data is an expensive task, as data needs to be gathered and cleaned to produce good results in training.
There is some emerging work on generating synthetic datasets by using large language models as data augmentation tools.
But even if the fine-tuning data quality is sufficient, it is still a challenge to make an LLM expert in a specific domain.
Rather than learning new knowledge, fine-tuning is best suited to guide the model toward a certain answering style.
Furthermore, fine-tuning models with billions of parameters is only possible on expensive hardware and therefore not feasible for on-demand tasks.

Following up on the challenges of fine-tuning researchers have searched for ways to optimize prompts to get the best model performance.
These methods make use of a phenomenon called in-context learning.
In-context learning describes the observation that giving a large language model more context surrounding the prompt can drastically improve the response quality.
\cite{Wei2022} found that adding a sentence that suggests step-by-step thinking to solve a problem makes the model better at solving logical questions.
For \gls{gpt} models, \autocite{Xu2023} showed an improvement after giving the language model a hint that it is an expert in a certain topic.
The findings in the area of prompting techniques and in-context learning are suggesting
that prompt optimizations have a high potential of getting the most out of large language models.

A more promising approach is \gls{rag} \cite{Lewis2020}.
Before generating an answer to a prompt, a vector database is searched for relevant information.
The prompt then includes the information of the returned documents as context.

\section{Retrieval Augmented Generation}
\label{sec:rag}

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[every node/.style={block}, every path/.style={->}]
        \node (u) {User};
        \node[right=of u] (q) {Query};
        \node[right=of q] (DB) {DB};
        \node[right=of DB] (qc) {Query + Context};
        \node[right=of qc] (LLM) {LLM};
        \node[right=of LLM] (a) {Answer};
        \draw (q) -- (DB);
        \draw (DB) -- (qc);
        \draw (qc) -- (LLM);
        \begin{scope}[on background layer]
            \node[fit={(q) (DB) (qc) (LLM)}, label={RAG Pipeline}] (rag) {};
        \end{scope}
        \draw (u) -- (rag);
        \draw (rag) -- (a);
    \end{tikzpicture}
    \caption{Instead of directly answering the user prompt with an \gls{llm},
        a \gls{rag} system leverages context to generate better answers.
        After receiving the user prompt, it is used to retrieve relevant documents from a database.
        These documents are appended to the language model prompt as context.
        The model is instructed to answer only using the provided context.}
    \label{fig:rag_flowchart}
\end{figure}


\Glspl{llm} can embed a large amount of knowledge into the weights during the pre-training phase.
It is possible to generate good answers for different kinds of prompts.
But when it comes to specific domain knowledge, the exactness of \gls{llm} starts to degrade.

When trying to prompt models about very specific topics common problems such as hallucinations start to show.
Not only does the model generate wrong information, it does so with strong confidence.
A promising method to make the model answer based on specific sources is called \glsfirst{rag}.

The \gls{rag} method combines an information store with in-context learning.
A corpus of documents is stored in a database.
When the user prompts the system, the database is queried with that prompt matching documents are returned.
These documents are then included in the prompt to the \gls{llm} as context.

To store the information a vector database is used.
A vector database uses embedding models to embed each document into a high-dimensional vector space.
After a user query, the query string gets embedded by the same model,
and then a metric such as the cosine similarity is used to find semantically close documents.
The top-$k$ documents are then returned.

\section{Agent}

In this section, I will describe in detail how the \gls{ir} agent is works.


\subsection{Main Loop}

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \node[block] (user) {User};
        \node[block, right=of user] (task) {Task};
        \node[block, right=of task] (step) {Step};
        \node[block, above right=of step] (LLM) {LLM};
        \node[block, below right=of step] (output) {Output};
        \node[block, below right=of LLM] (ability) {Ability};

        \draw[->] (user) -- (task);
        \draw[->] (task) -- (step);
        \draw[->] (step) -- (LLM);
        \draw[->] (LLM) -- (ability);
        \draw[->] (ability) -- (output);
        \draw[->] (output) -- (step);
        \draw[->, dotted] (output) -- (user);

    \end{tikzpicture}
    \caption{The \gls{ir} agent receives the user prompt at the start of each cycle.
        Next, the step prompt is constructed with the action history, list of abilities and extra information.
        After the LLM responds to the step prompt, the proposed action is executed
        and the step output is presented to the user.
        The loop ends if the LLM proposes the 'finish' ability.}
    \label{fig:agent_loop}
\end{figure}

The agent program is defined in the agent step execution function.
Every time a new step request is given this function is called.
For the first user input, a task is created,
then the agent proceeds to complete steps until the task goal is reached.
A single cycle of the agent follows a fixed set of actions.
First, the step input is updated for the current step.
The user can give updated instructions in each step to further guide the agent.
If no new input is given, the agent uses the input of the parent task as the step input.

After the input is updated the agent constructs the message history for the language model.
The first message is always the same system format specification.
The agent uses the default Forge agent format shown in \autoref{lst:system-format}.
The second message is the populated step prompt, which describes the current perceived environment of the agent.
When the step prompt is constructed, it is sent to the language model.
The generated answer is parsed into the thoughts and ability parts.

The response is parsed into the thoughts of the LLM and the proposed ability.
If a valid ability is proposed, it is executed.

\subsection{Agent Memory}

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[node distance=2cm]
        \node[block=emph red] (workspace) {Workspace};
        \node[above=of workspace] (user) {User};
        \node[ability, below=of workspace] (retrieve) {Retrieve};
        \node[ability, left=2.5cm of workspace] (ingest) {Ingest};
        \node[ability, right=2.5cm of workspace] (answer) {Answer};
        \node (DB) at (ingest |- retrieve) {DB};
        \node[below=of answer] (LLM) {LLM};

        \draw[->] (user) -- node[left] {place PDF} (workspace);
        \draw[->] (workspace) -- node[above] {read PDF} (ingest);
        \draw[->] (ingest) -- node[left] (chunks) {add chunks} (DB);
        \draw[->] (DB) -- node[above] {top chunks} (retrieve);
        \draw[->] (retrieve) -- node[left, align=center] {save\\context} (workspace);
        \draw[->, transform canvas={yshift=1mm}]
        (workspace) -- node[above, align=center] {get context} (answer);
        \draw[->, transform canvas={yshift=-1mm}]
        (answer) -- node[below, align=center] {save answer} (workspace);
        \draw[->, transform canvas={xshift=1mm}]
        (answer) -- node[right, align=center] {prompt\\with context} (LLM);
        \draw[->, transform canvas={xshift=-1mm}]
        (LLM) -- node[left] {answer} (answer);
    \end{tikzpicture}
    \caption{The \gls{rag} agent manipulates the \textcolor{emph red}{workspace}
        folder with \textcolor{emph blue}{abilities}.
        To ingest a PDF, the user has to place the file into the workspace.
        After chunking the PDF, the chunks are ingested into the vector database.
        With the retrieval ability,
        the agent can retrieve the top five chunks for a query
        and save them in a workspace file.
        The retrieved documents are read from the workspace by the Answer ability.
        The generated answer is then written back into a file in the workspace.}
    \label{fig:abilities_workspace}
\end{figure}


There are three places that the agent stores memories, the workspace,
the vector database and the prompt context.

The workspace is used to store larger amounts of information between agent steps.
It can be viewed as a paper where a person writes notes on while working.
Abilities can modify anything that is inside the workspace.
For each task started by a user, the agent creates a new workspace.
Access from abilities to the workspace is shown in \ref{fig:abilities_workspace}.
After retrieving the top documents from the vector database, the agent stores
them in workspace text file.
When the answering ability is chosen, the workspace file is read and used to populate
the \gls{rag} template.
After the \ref{llm} answers the \gls{rag} prompt, the answer is written back
into another workspace file.

The vector database can be viewed as a storage for external information,
similar to a library.
It is used to store large amount of information that is indexed upon addition
to the database.
A user can prompt the agent to ingest a PDF into the vector database with an ability.
This starts the indexing process described in section~\ref{sec:rag}
When the retrieval ability is activated, the agent retrieves the top $5$ documents
from the vector database.

The third storage is for storing information about the internal state of the agent.
Here, the agent keeps track of a step history for each task.
Each history includes the \gls{llm} output with the thoughts and proposed ability.
Furthermore, all ability outputs are saved.
This information is used to build the step history that is appended to the step prompt.

In summary, there are three places where information are stored

\subsection{Step Prompt}

\lstinputlisting[
    style=Code,
    linerange={1-25, 36-41},
    float=tp,
    caption={The step prompt is the core of each agent step.
            It describes the current environment representation of the agent.
            First, the current task is presented to the agent.
            Next, available resources and abilities are described.
            Finally, some hints for best practices are denoted.
            In this example, there is no step history as this is the first step.},
    captionpos=b,
    label={lst:step_prompt}]{include/code/example_step_prompt.txt}

The main step of the agent is to populate its step prompt template
As denoted in \autoref{sec:agents}, an agent perceives its environment through sensors.
Our agent describes the current environment in the step prompt.
An example step prompt is shown in \autoref{lst:step_prompt}.
It includes the current step input and instructions on how to answer.
The available resources are denoted, and the action history is appended.

If the agent is in its first step, the action history is empty.
Otherwise, a list of the previous actions is compiled to give the agent a sense of its current state inside the task context.
Each entry has the proposed ability with its parameters and the ability output.
If the ability produced an error, this is also denoted.
In the best practices section, the agent is prompted to react to errors and to not use the same action with the same arguments again.

In addition to the action history, the available abilities are added to the prompt.
The abilities enable the agent to act out its decisions in the environment.
For each ability, the name, parameters, return type and a short description is given.

\subsection{Step Prompt Answer}

The \gls{llm} receives the step prompt and generates an answer containing
thoughts about the current step and an ability proposal.

% TODO: add step prompt answer example

\subsection{Abilities}

In \autoref{sec:agents}, I described how agents use actuators to perform actions in the environment.
The IR agent is a software agent that uses functions to modify its workspace.
In particular, it has abilities that enable retrieval augmentation capabilities.
These abilities will be described in the following.

\begin{description}
    \item[Ingest] The \emph{Ingest} ability should be used
          when a document should be a source of information for the agent.
          A file with the specified filename has to exist in the agent workspace before calling this function.
          This ability converts the PDF into plain text, creates text chunks of the same length
          and calls the functions to embed them into the vector database.
    \item[Retrieve] The \emph{Retrieve} ability accepts to query string and an output file path.
          With the query string, the agent queries its memory and gets the most relevant documents for it.
          The documents are formatted and saved to the specified file path.
    \item[Answer] This ability uses the contents of a text file to populate the augmented generation template.
          The populated template is then sent to the LLM to generate an answer based on that context.

\end{description}

As the LLM tends to choose web search abilities to collect information, I removed the corresponding abilities.
The agent is therefore forced to retrieve information from local sources.

\subsection{Document Ingestion}

To enable \gls{rag},
the agent can ingest documents into its vector database.
The “ingest” ability can be called with a PDF filename as the argument.
For the ability to succeed, a PDF file with the exact filename has to be present
in the agent workspace.
The user has to manually place the file in the workspace before prompting the agent.
If a PDF is found, it is converted into plain text.
Then, the text is split up into smaller chunks that can be mapped to embeddings
with an embedding model.
The \gls{sentsplitter} utility from the \gls{llamaindex} framework \cite{zotero-255} is used,
to create the chunks.
The splitter splits up a text recursively and then joins words back together to
generate chunks of similar length while respecting sentence borders.

As PDF is an unstructured format, the conversation to plain text is not trivial.
In addition to the main content, the article contains extra information like page numbers,
references or footnotes.
These extra elements can split paragraphs that span two pages.
After splitting up the text with the SentenceSplitter, these elements are still
present in the chunks.
An exemplary chunk is shown in listing~\ref{lst:two_page_chunk}.
For the following embedding step,
this can hinder the semantic accuracy of the embedding.
After splitting the text into chunks, they are added to the \emph{ChromaDB}
vector database \cite{zotero-176}.

\subsection{Answering with Context}

% TODO: describe LLM call to answer with context.

\end{document}