\documentclass[../../main.tex]{subfiles}
\begin{document}
The left-to-right nature of the transformer encoder suits text generation tasks that only depend on previous tokens.
But there are dependencies between words in both directions, which means a decoder only model can not capture all relations.

The big amount of knowledge learned in the pre-training phase is encoded in embeddings.
Embeddings are high dimensional vectors that represent a sequence of words or tokens.
Documents that are semantically similar are represented with embedding vectors that have low distance, while semantically different documents have a high distance.

Embedding models like BERT \cite{Devlin2019} use the embeddings from the encoder section of a transformer network.

As the embeddings
Embeddings models can be fine-tuning on different downstream tasks to create embeddings that are optimized for the specific task domain.
\end{document}