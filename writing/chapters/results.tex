\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{Results}
\label{ch:results}

% TODO: this needs work
In chapter~\ref{ch:benchmarks} I described the creation for a benchmark
to test the \gls{ir} agent created in chapter~\ref{ch:agent}.
This chapter presents the results.
First, the results for four different chatbot services will be compared
section~\ref{sec:benchmark_results}.
I compare the result of GPT-3-Turbo, GPT-4-Turbo, Perplexity AI and the \gls{ir}
agent.
The small size of the dataset does not allow general statements about
answer quality for the \gls{ir} agent.
Therefore, section~\ref{sec:subjective_evaluation} looks at exemplary question and answer
pairs to compare the answer quality.
While generating the answer for the benchmark, the \gls{ir} agent had some failed runs.
The patterns that lead to failed runs are described in section~\ref{sec:pof}.

While working with \gls{autogpt} and developing the \gls{ir} agent,
a couple of weaknesses of current \gls{llm} agent systems became visible.
The most important challenge for \gls{llm} agents is the non-deterministic generation of large language models.
It makes it significantly harder to build a software system around it because every time the language model is called,
there has to be a fallback mechanism in case something fails.
For fixed pipeline systems this is true as well,
but error handling is easier as the task steps are known beforehand.

\section{Benchmark Results}
\label{sec:benchmark_results}

In chapter~\ref{ch:benchmarks} a benchmark to test the \gls{ir} agent was created.
The benchmark consists of a small question and answer with $10$ pairs.
Testing the \gls{ir} agent was done manually using the \gls{autogpt} web interface.
I pasted the question in the prompt window without any extra guidance for the agent.

As services to compare the agent to I chose the free version of Perplexity AI,
GPT-3.5-Turbo and GPT-4-Turbo.
To generate the answers of Perplexity AI, I pasted the question the prompt window in the
same way as with the \gls{ir} agent.
For the \gls{gpt} models a used the API functions.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar,
                ymin=0,
                ymax=0.7,
                legend cell align={left},
                width=\textwidth,
                height=\textheight/3,
                ylabel={Cosine Distance},
                xlabel=Question,
                bar width=0.25cm,
                tickwidth=0pt,
                y axis line style={opacity=0},
                symbolic x coords = {
                        Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9
                    }
            ]
            \addplot [draw, pattern={Lines[angle=45, distance=4pt]}] coordinates {
                    (Q1, 0.32)
                    (Q2, 0.59)
                    (Q3, 0.47)
                    (Q4, 0.31)
                    (Q5, 0.19)
                    (Q6, 0.26)
                    (Q7, 0.23)
                    (Q8, 0.41)
                    (Q9, 0.29)
                };
            \addplot [draw, pattern=crosshatch dots] coordinates {
                    (Q1, 0.32)
                    (Q2, 0.45)
                    (Q3, 0.46)
                    (Q4, 0.35)
                    (Q5, 0.22)
                    (Q6, 0.23)
                    (Q7, 0.24)
                    (Q8, 0.31)
                    (Q9, 0.33)
                };
            \addplot [draw, pattern={Lines[angle=-45, distance=4pt]}] coordinates {
                    (Q1, 0.30)
                    (Q2, 0.52)
                    (Q3, 0.45)
                    (Q4, 0.31)
                    (Q5, 0.18)
                    (Q6, 0.21)
                    (Q7, 0.24)
                    (Q8, 0.37)
                    (Q9, 0.34)
                };
            \addplot [draw, pattern=horizontal lines] coordinates {
                    (Q1, 0.30)
                    (Q2, 0.63)
                    (Q3, 0.38)
                    (Q4, 0.30)
                    (Q5, 0.22)
                    (Q6, 0.29)
                    (Q7, 0.17)
                    (Q8, 0.33)
                    (Q9, 0.28)
                };
            \legend{GPT-3.5-Turbo, GPT-4-Turbo, Perplexity AI, IR Agent}
        \end{axis}
    \end{tikzpicture}
    \caption{Semantic distances between the gold standard answer and the generated answer.
        The semantic distance is calculated as the cosine distance of the sentence embeddings.}
    \label{fig:benchmark_results}
\end{figure}

The benchmarking results are shown in figure~\ref{fig:benchmark_results}.
We can observe, that there is no visible pattern in the results.
For each question different tools perform better or worse.

\section{Subjective Evaluation}
\label{sec:subjective_evaluation}

In the case of natural language answering, judging answer quality depends heavily
on the user of the answering system.
I will point out some questions with good answers and some with bad answers,
and compare these across the different services.

\section{Points of Failure}
\label{sec:pof}

Using large language models as agent controllers is a recent idea, and therefore there is a lot of experimentation left to do.
A challenge when developing such agents is the natural language interface that language models communicate with.
While it makes describing the task easier for humans, it complicates the internal communication in the software.
For example, \gls{autogpt} wants a certain system format that the \gls{llm} should respond in.
While this works most of the time, it is not guaranteed that the answer complies with the format.
A small deviation such as a missing bracket can break the parsing process.
Therefore, a lot of effort and time is put into creating better tools and frameworks that handle such mistakes,
instead of doing actual research on the agent performance.

Another aspect is the non-deterministic generation of large language models. % TODO: lookup temperature of llms in this context
For the same prompt, large language models can generate a different answer.
This answer will probably be semantically similar to the previous one,
but when a model has to choose between two abilities,
this small difference can decide if the agent succeeds or fails at completing the task.
Like earlier, developers have to spend time handling these cases by re-prompting several times or putting fallback actions in place.

For a lot of tasks, a static pipeline is enough to meet the user demands.
Almost all retrieval tools that work in production use a fixed pipeline.

\subsection{Ability Choice}

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[
            ab/.style={ability, minimum height=2cm, align=left}]
        \node[minimum height=2cm, align=left] (prompt) {What is\\a 'qersu'?};
        \node[ab, right=of prompt, label={retrieve}] (step1) {Chunk1\\\dots\\Chunk5};
        \node[ab, right=of step1, label={read\_file}] (step2) {Doc 1: Chunk 1\\\dots\\Doc 5: Chunk 5};
        \node[ab, right=2cm of step2, label={write\_file}] (step3) {VGhlIH\\NlYXJja\\CBmb3I\\\dots};
        \node[right=of step3] (finish) {finish};
        \draw[->] (prompt) -- (step1);
        \draw[->] (step1) -- node[above, align=center] {file} (step2);
        \draw[->] (step2) -- node[above, align=center] {prompt\\context} (step3);
        \draw[->] (step3) -- (finish);
    \end{tikzpicture}
    \caption{An agent run that failed because of an incorrect action choice.
        The resulting answer is just random characters.}
    \label{fig:bad_agent_run}
\end{figure}

The agent sometimes chose abilities that were not suited to progress towards the goal.
When the agent had read and write abilities present, sometimes the agent chose to
read the file containing the retrieved context for a query.
The correct choice would have been to process with the Answer ability.
In some instances, choosing the wrong ability leads to useless answers,
as shown in figure~\ref{fig:bad_agent_run}.

A solution to this problem is to either remove the read and write abilities
or to guide the agent with explicit instruction in the prompt on which ability to use.
For a true autonomous agent, these solutions are not ideal, because we want the
agent to make the decisions with minimal user intervention.
Fixes like removing abilities bring the agent closer to fixed pipeline solutions.
Another solution could be to retry the same or modified prompts.
This is feasible if the user does not demand fast responses.
In the case of \gls{ir}, the agent should answer reasonably fast and
give correct answers.
Therefore, I opted to remove all abilities that were not needed for \gls{rag}.

Furthermore, the model choice is important for decision-making.
GPT-4 is a lot better than GPT-3 at making good decisions, following tasks and providing
criticism and reasoning.
For this reason, GPT-4 was used as the agent controller \gls{llm}.

\subsection{Answer Format}

After receiving the answer to the step prompt, the agent has to parse it.
The parsing step can only work if the answer is generated in the exact format specification.
It is not certain that an \gls{llm} adheres to the system format.
The capabilities in answering in a structured format given by the user differ between models.
% TODO: sources
Larger models like GPT-4 are better than smaller ones like GPT-3, but also cost more.
Other models improve at structured answers with fine-tuning
but might lose capabilities in other areas as a result.

% TODO: sources
There are solutions, that try to catch common mistakes from \glspl{llm}
such as missing brackets or missing commas in JSON.
While some mistakes can be mitigated with this approach, it is not feasible
for large-scale applications as there are too many where the format can break.
Similar to the decision-making mistakes, another solution is to retry
and hope for a parsable answer.


\end{document}