\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{An Analysis of AutoGPT}
\label{ch:autogpt}

After its initial release,
\gls{autogpt} quickly gained a lot of traction in the open-source community.
The project tries to ``make \gls{gpt} fully autonomous'',
by using GPT as an agent ``brain''.
\gls{autogpt} has sparse documentation that is quickly outdated,
as the project is unfinished and often changes.
Therefore, I will give a brief introduction to the main components of
\gls{autogpt} and analyze the project in the context of \gls{ir} to build
the foundations for the agent creation chapter.

Section~\ref{sec:components} introduces the main components of the \gls{autogpt} project.
What initially grabbed the attention of many people is not the whole project,
but the \emph{\gls{autogpt} agent} component of \gls{autogpt}.
The project later introduced the \emph{Forge agent},
an agent framework to build custom agents.
It defines a minimal agent without logic
that abstracts away all the boilerplate.
Additionally, the \gls{autogpt} project contains a benchmarking system.
The \emph{benchmarking system} can be used to build level-based test suites,
to test the capabilities of an agent.

In section~\ref{sec:analysis}, the project and its components
are analyzed in the context of information retrieval.
The \gls{autogpt} agent is built to be a ``generalist agent'' that can do
a variety of tasks and therefore is complex.
Because of the complexity,
I opted to create a custom Forge agent to research the use of \gls{autogpt} for \gls{ir}.
To work with local data, the agent has abilities to read text files
and put the output into the prompt.
This defines the key limitation of the agent, for longer texts the context
window of the \gls{llm} is too short.
Therefore, I chose to use a \gls{rag} approach in combination with \gls{autogpt}.

\section{Main Components}
\label{sec:components}

In this section, the main components of the \gls{autogpt} project are introduced.
The project provides two agents, the \gls{autogpt} agent and the Forge agent.
While the \gls{autogpt} agent is built to be a ready-to-use ``generalist agent'',
the Forge agent provides a minimal agent for developers that want to implement
a custom agent.
Furthermore, \gls{autogpt} comes with a benchmarking system to test agent abilities.

To make collaborating on agent projects easier,
the open-source community created the \emph{Agent Protocol} \cite{zotero-111}.
The Agent Protocol defines an API schema
that handles the communication with an agent.
On a high level,
the protocol defines endpoints to create tasks and trigger the next step for a task.
The two important concepts of the agent protocol are tasks and steps:

\begin{description}
      \item[Task] A task describes a goal for the agent.
            A task has an input prompt and contains a list of steps.
      \item[Step] A step describes a single action of the agent.
            Every step has to be linked to a parent task and has an input.
            Additionally, a variable signals if this is the last step.
            If this variable is false,
            the next step is requested automatically.
\end{description}
The \gls{autogpt} agent and Forge agent both implement the agent protocol.

\subsection{AutoGPT Agent}
\label{subsec:autogpt_agent}

The \gls{autogpt} agent made up the whole project when \gls{autogpt} was first released.
It is the default agent and comes with pre-defined logic.
Currently,
the \gls{autogpt} agent is optimized for OpenAI \gls{gpt} models.
The prompts are tailored in ways
that benefit the characteristics of GPT-3 and GPT-4.
The official documentation describes the default agent as
\begin{displayquote}[\citetitle{zotero-189} \cite{zotero-189}]
      A \emph{generalist} agent, meaning it is not designed with a specific task in mind.
      Instead, it is designed to be able to execute a wide range of tasks across many disciplines,
      as long as it can be done on a computer.
\end{displayquote}
This generalist design can be seen in the list of available abilities.
\begin{description}
      \item[Code Execution] The agent can execute arbitrary Python code given as a
            string or a file.
            The execution is contained in a docker container
            that can access the agent workspace.
      \item[File Operations] To access files in the agent workspace,
            it has abilities to list, read, and write files.
      \item[Image Generation]
            The agent can make calls to different image-generation services
            that transform a prompt into an image.
      \item[Web Search]
            Web search is the most common application of the default agent.
            The agent has abilities to query a search engine
            and scrape website content using a headless browser.
      \item[Other]
            Further abilities are present in the default agent.
            The user can be prompted to give more information about the task,
            and the system time can be accessed.
            When the agent ends its run, a finishing ability is chosen.
\end{description}

During the development of the default agent,
different abilities were present and were then removed again.
For example, efforts were made to introduce long-term memory to the agent.
There were abilities to save agent conversations into a database for later retrieval.
The idea was to improve the agent's performance by using past conversations as context.
However, experiments showed that using retrieved long-term memories of
conversations did not improve performance substantially \cite{Pwuts}.

The \gls{autogpt} agent is modeled after the classic agent architecture.
After the start,
the user is asked to enter a task the \gls{autogpt} agent should perform.
Then the agent enters a loop of prompting the LLM,
executing the proposed action
handling the result of the action, and updating the agent state.

\subsection{Forge Agent}
\label{subsec:forge_agent}

The Forge SDK handles the boilerplate code that implements the agent protocol.
On running, the server with the corresponding endpoints gets started,
and the agent can be used over the API endpoints.
\Gls{autogpt} also comes with a chatbot web app
that builds the appropriate HTTP requests to the agent endpoints.
The user of the Forge agent has to create the actual agent logic,
create custom prompt templates for the used model,
and add abilities to interact with external resources.
Because the Forge agent is still under development and by no means a polished product,
some internals also need to be tweaked to achieve the desired agent behavior.

By default,
the Forge agent comes with abilities to read and write text files
and to search a search engine as well as scraping a webpage.
Each ability is specific by a name that the \gls{llm} can use to call it.
Additionally, a short description of what the ability does,
input parameters, and return types are described.
The information about an ability is formatted into a string before adding it to the step prompt.

\begin{description}
      \item[File System] The file system abilities allow operations on files located in the workspace of the agent.
            The agent can only operate in the defined workspace, this prevents unwanted effects when the agent proposes unexpected actions.
            Abilities to read, write and list files are present.
            All the abilities have a path parameter that the large language model has to populate when generating a step proposal with actions from this category.
      \item[Web] The web search functionality is split up into abilities to call a search engine and to read a webpage
            The web search ability requires a search string that is sent to the engine.
            For the webpage ability a URL is needed, and if specific information should be extracted the LLM also has to provide a question.
      \item[Finish] The “finish” ability terminates the agent loop.
            The agent is asked to choose this ability if the initial user prompt can be answered and give a reason.
\end{description}

\begin{Code}[
            caption={The system format of the Forge agent.
                        The language model is asked to only answer in this format.
                        The model generates thoughts before creating the output for the user (speak).
                        Thoughts include reasoning, a plan, and criticism.
                        After the model generates the thoughts,
                        it generates an ability proposal with the corresponding arguments.},
            label={lst:system-format},
            captionpos=b,
            float=tp]
Reply only in JSON with the following format:

{
      "thoughts": {
            "text":  "thoughts",
            "reasoning": "reasoning behind thoughts",
            "plan": "- short bulleted
                     - list that conveys
                     - long-term plan",
            "criticism": "constructive self-criticism",
            "speak": "thoughts summary to say to user",
      },
      "ability": {
            "name": "ability name",
            "args": {
                  "arg1": "value1", etc...
            }
      }
}
\end{Code}

The Forge agent comes with a built-in template engine that is used
to populate the prompts before sending them to the large language model.
Some templates are included by default:

\begin{description}
      \item[Task-Step] This template is used to create the prompt that is sent to the language model for each step.
            It includes the current task description and placeholders for extra information.
            The template always needs to be populated with the list of available abilities,
            allowing the language model to choose one of them.
      \item[System-Format] The system format defines how the model should respond to prompts.
            The Forge system format is depicted in \autoref{lst:system-format}.
            The responses of the model need to be parsed according to this definition.
            Language models have different capabilities in answering in a structured manner,
            so the format has to be tuned for every model.
      \item[Techniques] Techniques is a collection of prompting techniques that have been shown to improve generation quality.
            By default, the Forge agent has templates for few-shot, expert, and chain-of-thought prompting.
\end{description}

The core of an agent is its logic.
The Forge agent comes without any logic,
its default behavior is to write a boilerplate text into a file in the workspace.

\subsection{Benchmarking System}
\label{subsec:benchmarking}

To evaluate \gls{autogpt} and other agent systems that implement the agent protocol,
the \gls{autogpt} project has implemented a benchmarking system.
The system consists of a set of tasks that the agent has to complete.
The tasks are designed to test different aspects of the agent and are divided into different topics.
Some tasks depend on the previous successful completion of other tasks.
A task consists of an input prompt and an expected output.
The output is defined by certain words that should be contained.

\section{Analysis for Information Retrieval}
\label{sec:analysis}

Before I describe the use of \gls{autogpt} as an \gls{ir} agent in the next chapter,
I will reason about my choices in this section.

The \gls{autogpt} agent was not built with \gls{ir} as a primary objective in mind.
As stated in section~\ref{sec:components}, it is designed to be a \blockquote{generalist agent}.
The \gls{autogpt} agent has abilities to retrieve information from the web.
With web searching and website scraping abilities, the agent can
look up and extract information about a given topic.
For \gls{ir} from local data sources, there are no specific abilities present.
The agent can simply read text files and include the content in the next step prompt.
This means that every other file type has to be converted into plain text.
Another inherent problem with this approach is the limited context window size of the language model.
If the relevant information is contained in a long text, including the full text
in the prompt will not be possible.
Furthermore, the \gls{autogpt} agent has a lot of other abilities that are irrelevant
for \gls{ir}.
While they can be disabled, they still increase the complexity and points where the agent
can break.
This leads to more difficulties when using the agent for a specific task
like local \gls{ir}.

The Forge agent has similar default abilities for retrieval of information from the web,
as well as reading and writing abilities.
Therefore, the same problems with context window length apply here.
As the agent only contains boilerplate implementation for the agent protocol
but no default logic,
it is better suited as a starting point for a custom \gls{ir} agent.

The benchmarking system can be used to test the abilities of agents.
The level-based system is put in place to save money when testing,
as later-stage tests, do not get executed if a required test in the earlier
stage fails.
To evaluate \gls{ir} systems, a larger dataset of \gls{qa} pairs is often used.
For large dataset evaluations, the benchmarking system is not suited.
It is built towards testing if an agent \emph{can} achieve a goal
rather than \emph{how good} the produced answer is.

These arguments lead to the conclusion that to examine the use of \gls{autogpt}
for \gls{ir}, it is best to create a custom agent built on the Forge agent.
To enable local \gls{ir} over large documents specific abilities will be introduced.
The creation of the agent is described in the next chapter.


\end{document}
