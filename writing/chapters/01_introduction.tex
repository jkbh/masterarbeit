\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{Introduction}
\label{ch:introduction}

For a long time, \gls{ir} has been a heavily researched topic in computer science \cite{Singhal2001}.
Retrieving information from a large set of documents is an essential task in many domains.
For example,
a researcher might want to discover relevant parts of a new article for a given topic.
Ideally, he can chat with an \gls{ir} system in natural language
and receive correct information from the article.
In recent years, there has been a large increase in language modeling capabilities.
\Glspl{llm} have been used for a variety of applications such as chatbots,
writing helpers, and coding assistants.
There are different approaches to leverage \gls{ir} for \gls{llm} systems such as \gls{rag}.
A \Gls{rag} system retrieves documents from a database before using them as context
for the \gls{llm} generation.
This method enables the on-demand inclusion of information into the answer generation.
Recently, efforts were increased in using \glspl{llm} as agent controllers.
In comparison to static systems, agents act autonomously toward achieving a goal.
They constantly perceive the environment through sensors and perform actions
through actuators.
For the researcher,
this could be the goal of gathering all relevant text passages about a topic from
multiple articles.
An agent could then autonomously proceed to gather the information.
\Gls{autogpt} is an open-source project that uses an \gls{llm} to provide agent
actions.
While there are experiments with \gls{autogpt} for navigating the web \cite{Yang2023},
using it for \gls{ir} over local documents has not been researched yet.

This thesis presents an \gls{llm} agent built with \gls{autogpt},
that uses \gls{rag} to answer user prompts.
The agent is benchmarked with a small set of \gls{qa} pairs from a humanities journal.
A qualitative evaluation of the \gls{ir} agent benchmark is given in the results.

\section{Contributions of this Thesis}

This thesis has three main contributions:
\begin{enumerate}
    \item An analysis of \gls{autogpt} in the context of \gls{ir}.
    \item A custom \gls{ir} agent that uses \gls{rag} for \gls{ir}.
    \item A small humanities journal \gls{qa} benchmark to test the \gls{ir} agent.
\end{enumerate}

In the analysis, I conclude that to research \gls{ir} applications for \gls{autogpt},
an adaption of the default abilities for \gls{rag} is needed.
Due to the complexity of the default agent,
I further reason that it is best to build a custom agent with the baseline Forge agent.

The custom \gls{ir} agent is built with the Forge agent baseline included in the
\gls{autogpt} project.
A custom agent logic is implemented to define the agent loop.
The agent has abilities to ingest a PDF into its memory, retrieve documents for it,
and use them to answer a user query.
Rather than following a fixed pipeline, the agent chooses these actions to
dynamically perform the \gls{rag} steps.

The benchmarking dataset contains eight \gls{qa} pairs from a humanities journal.
I used GPT-4 to generate candidate questions and then handpicked answers from
the journal articles.
The \gls{ir} is compared to GPT-3, GPT-4, and Perplexity AI
using cosine similarity between the ground truth and generated answers.
No service outperforms the other ones.
A qualitative evaluation of the agent explains the common failure points.
The main problems of the agent are irrelevant retrieval results,
inconsistent \gls{llm} response formats, and choosing the wrong abilities.

\section{Related Work}

With the rise of \glspl{llm}, they have been introduced into different domains.
\Gls{llm} agents are a promising way to fulfill user goals autonomously,
hinting in the direction of more helpful computer assistants.
Language models have also seen rising popularity in \gls{ir} applications.
New search engines like Perplexity AI leverage language models
to present the retrieved data to the user.
In the following,
notable work that is of interest for this work is listed.

After \glspl{llm} like \gls{gpt3} and then \gls{gpt4} were made public,
researchers started testing different approaches to integrate them into agents.
In particular, the introduction of tools to extend the language model capabilities.
Toolformer \cite{Schick2023} trains a language model to use different APIs in a self-supervised way.
For example, the model can use a calculator to externalize math problems,
a task category that \glspl{llm} struggle with.
HuggingGPT \cite{Shen2023} uses \gls{gpt} repeatedly in multiple rounds.
For each step, a Huggingface model can be selected by the model to answer the prompt.
In a virtual social setting, multiagent communication was demonstrated in \autocite{Park2023}.
They created agents that live together in a virtual village,
and have different goals and communicate through natural language.

Built on top of research experiments with tool usage,
multiple applications of \gls{llm} agents have been presented.
\Gls{autogpt} \cite{SignificantGravitas2023} is an attempt making GPT autonomous.
The \gls{autogpt} agent has a variety of abilities such as web search,
image generation, code execution, and file operations.
It uses GPT-4 to generate an action proposal from a list of abilities.
An introduction to \gls{autogpt} is given in \autoref{ch:autogpt}.
I will use the included Forge agent to build a custom agent in this thesis.

Besides \gls{autogpt}, other \gls{llm} agent projects have been published.
BabyAGI is a minimal task-driven autonomous agent \cite{Nakajima2023}.
With OpenAI Assistants~\cite{zotero-195},
users can build custom assistants
that can use OpenAI models, tools, and knowledge to answer user queries.
Perplexity AI \cite{zotero-197} is an online service that uses sources from
the web to generate answers to user queries.
The generated question contains links to the documents that were used as a source.
The service could be viewed as an agent, as it takes the user input and autonomously
gathers relevant information.
Furthermore, the user can update the task description with follow-up prompts
to Perplexity AI.

\section{Structure of this Thesis}

First, I will introduce the main backgrounds
that are relevant to this work in \autoref{ch:backgrounds}.
The steps of an \gls{ir} system are explained, as well
as different strategies to evaluate such systems.
Furthermore, I will give an overview of the concept of an \emph{agent} and its main components.
Additionally, the language modeling section covers the base
to understand the current \glspl{llm}.
I will go from the introduction of the transformer architecture to current chat models like ChatGPT.
Finally, the combination of language models and agents to \emph{\gls{llm} agents} is introduced.
Chapter~\ref{ch:autogpt} contains an introduction to \gls{autogpt} and its core elements,
as well as an analysis of its current \gls{ir} capabilities.
First, an overview of the \gls{autogpt} project and its components is given.
I will then explain the default agent in more detail,
looking at how the \gls{llm} agent ideas are implemented.
After gaining an understanding of the \gls{autogpt} project,
we will then look at its default capabilities for \gls{ir} tasks.
In \autoref{ch:agent},
a custom \gls{autogpt} agent that uses \gls{rag} for \gls{ir} is presented.
I will introduce the concept of \gls{rag} first, then give a detailed explanation
of the \gls{ir} agent.
In \autoref{ch:benchmarks}, will deal with how \gls{llm} agents can be benchmarking
and present a test dataset for the \gls{rag} agent from \autoref{ch:agent}.
The benchmark uses handcrafted question-and-answer pairs from a scientific humanities journal.

\end{document}