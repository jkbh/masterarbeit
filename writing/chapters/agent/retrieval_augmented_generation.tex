\documentclass[../../main.tex]{subfiles}
\begin{document}

Large language models can embed a large amount of knowledge into the weights during the pre-training phase.
It is possible to generate good answer for different kinds of prompts.
"write about what are good prompts for llms".
But when it comes to specific domain knowledge, the exactness of LLMs start to degrade.

When trying to prompt models about very specific topics common problems such as hallucationations start to show.
Not only does the model generate wrong information, it does so with strong confidence.
A promising method to make the model answer based on specific sources is called retrieval augmented generaton(RAG).

The RAG method combines an information store with in-context learning.
A corpus of documents is stored in a database.
When the user prompts the system the database is queried with that prompt matching documents are returned.
These documents are then included in the prompt to the LLM as context.

To store the information a vector database is used.
A vector database uses embedding models to embed each document into a high dimensional vectorspace.
After a user query, the query string gets embedded by the same model, then a metric such as the cosine similarty is used to find semantically close documents.
The top-k documents are then returned.



\begin{itemize}
	\item what is a vectorestore compared to other databases
	\item how are documents stored and queried
	\item how is the information included in the prompt
\end{itemize}

\end{document}