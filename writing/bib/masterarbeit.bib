@InProceedings{Schiff2023,
  author    = {Schiff, Simon and Möller, Ralf},
  booktitle = {CHAI 2023},
  title     = {Persistent Data, Sustainable Information},
  year      = {2023},
  abstract  = {In almost all academic fields, results are derived from found evidence such as digitized objects stored at a repository. Deriving results from such repositories can be time and cost intensive, as data is often difficult to reuse. Guidelines such as FAIR (Findability, Accessibility, Interoperable, and Reuse) are intended to disseminate the proper archiving of research data such that, among others, archived data is easy to reuse. However, we argue that even if one follows FAIR guidelines, it is still challenging to derive results by reusing data. First of all, before reusing data from any repository, one needs to find the data. Search engines can index data stored at repositories, as data is associated with metadata as proposed by the FAIR guidelines. Deciding whether the data found is relevant usually requires downloading, extracting and visualizing the entire dataset which is time-consuming and costly. We propose that data need to be archived by associating it with metadata that determines how data can be prepared to support decision making. Metadata links data with executable code that can create an information system from associated data on demand. With our solution, data is easier to reuse, as one can decide whether found data is relevant. In addition, if the data is found to be relevant, our information system allows researchers to clearly refer to specific regions in the data for data governance.},
  file      = {:Schiff2023 - Persistent Data, Sustainable Information.pdf:PDF},
  groups    = {Private Papers},
  keywords  = {persistent data, sustainable information, research data, data management},
  rights    = {© 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
}

@InProceedings{Bender2023,
  author    = {Bender, Magnus and Schwandt, Kira and Möller, Ralf and Gehrke, Marcel},
  booktitle = {CHAI 2023},
  title     = {FrESH -Feedback-reliant Enhancement of Subjective Content Descriptions by Humans},
  year      = {2023},
  abstract  = {An agent in pursuit of a task may work with a corpus containing text documents. To perform information retrieval on the corpus, the agent internally maintains a model of the documents in the corpus. This model may contain annotations such as Subjective Content Descriptions (SCD)-additional data associated with different sentences of documents. In a our scenario, a human interacts with the information retrieval agent: The human sends a query to the agent, the agent uses its internal model to calculate a response and returns this response. However, the response may contain erroneous parts. Such errors, like faulty SCDs, may be send back to the agent by the human as feedback. Then, the agent can incorporate the feedback to improve its internal model. However, removing a faulty association of a sentence with an SCD in a previously trained model is a difficulty task-often the model needs to be retrained from scratch. To circumvent this, this paper presents FrESH an approach for Feedback-reliant Enhancement of Subjective Content Descriptions by Humans. Using FrESH the model keeps fresh and maintained with human feedback.},
  file      = {:Bender2023 - FrESH Feedback Reliant Enhancement of Subjective Content Descriptions by Humans.pdf:PDF},
  groups    = {Private Papers},
  keywords  = {Subjective Content Descriptions (SCDs), Text Annotation, Information Retrieval Agent, Incorporate Human Feedback, Incremental Model Adjustment, Information System},
  rights    = {© 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
}

@InProceedings{Asselborn2023,
  author    = {Asselborn, Thomas and Melzer, Sylvia and Aljoumani, Said and Bender, Magnus and Marwitz, Florian and Hirschler, Konrad and Möller, Ralf},
  booktitle = {CHAI 2023},
  title     = {Fine-tuning BERT Models on Demand for Information Systems Explained Using Training Data from Pre-modern Arabic},
  year      = {2023},
  abstract  = {Humanities scholars can use Large Language Models (LLMs) to simplify their work to analyse texts or recognise patterns in data. However, if a domain-specific problem needs to be solved, the existing models need to be fine-tuned. In the humanities, there is less training data available for fine-tuning, but there are increasing information systems with research data in that can be used for this purpose. To use data from information systems, humanities researchers need to be able to transform research data from information systems to training data so that it is suitable for training LLMs. However, assigning the correct labels (e.g., person, location, date) to the data can be challenging. This article describes how to fine-tune BERT models on demand for information systems explained using training data from pre-modern Arabic. The result we have achieved is that all archived research data can be used in a research data repository for fine-tuning models in short time and in a simplified way, i.e., without being an IT expert. Since the data was available in canonical form, it was possible to specify which fields could be assigned to which label by means of a manifest file. The results we obtained show that the fine-tuning process can be done in just a few minutes using a sample dataset and BERT. The Fine-tuning on Demand (FToD) process identified names of people, places, or dates that could not be recognised by the pre-trained model.},
  file      = {:Asselborn2023 - Fine Tuning BERT Models on Demand for Information Systems Explained Using Training Data from Pre Modern Arabic.pdf:PDF},
  groups    = {Private Papers},
  keywords  = {Fine-tuning on demand, BERT, pre-modern Arabic, manifest file Asselborn), 0000-0002-0144-5429 (S. Melzer), 0009-0004-8306-8621 (S. Aljoumani), 0000-0002-1854-225X (M. Bender), 0000-0002-9683-5250 (F. A. Marwitz), 0000-0002-6012-7711 (K. Hirschler), 0000-0002-1174-3323 (R. Möller)},
  priority  = {prio1},
  rights    = {© 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
}

@InProceedings{Bender2023a,
  author    = {Bender, Magnus and Braun, Tanya and Möller, Ralf and Gehrke, Marcel},
  booktitle = {KI 2023},
  title     = {LESS is More: LEan Computing for Selective Summaries},
  year      = {2023},
  abstract  = {An agent in pursuit of a task may work with a corpus containing text documents. To perform information retrieval on the corpus, the agent may need annotations-additional data associated with the documents. Subjective Content Descriptions (SCDs) provide additional location-specific data for text documents. SCDs can be estimated without additional supervision for any corpus of text documents. However, the estimated SCDs lack meaningful descriptions, i.e., labels consisting of short summaries. Labels are important to identify relevant SCDs and documents by the agent and its users. Therefore, this paper presents LESS, a LEan computing approach for Selective Summaries, which can be used as labels for SCDs. LESS uses word distributions of the SCDs to compute labels. In an evaluation, we compare the labels computed by LESS with labels computed by large language models and show that LESS computes similar labels but requires less data and computational power.},
  file      = {:Bender2023a - LESS Is More_ LEan Computing for Selective Summaries.pdf:PDF},
  groups    = {Private Papers},
}

@InProceedings{Redzuan2023,
  author    = {Redzuan, Nadja and Möller, Ralf and Gehrke, Marcel and Braun, Tanya},
  booktitle = {CHAI 2023},
  title     = {On Domain-specific Topic Modelling Using the Case of a Humanities Journal},
  year      = {2023},
  abstract  = {Topic modelling techniques have been an important tool for meaningful information retrieval. One prominent method, latent Dirichlet allocation (LDA), describes documents as distributions over topics and topics as distributions over words. Most applications of LDA focus on sets of tweets, news articles, wikipedia entries, or academic publications covering various topics. How LDA behaves with very domainspecific corpora is less well researched. Therefore, in this article, we apply LDA to infer hidden thematic structures from a corpus of an academic journal concerned with the studies of modern and ancient manuscripts. From this case study, we infer steps specific to dealing with domain-specific corpora.},
  file      = {:Redzuan2023 - On Domain Specific Topic Modelling Using the Case of a Humanities Journal.pdf:PDF},
  groups    = {Private Papers},
  keywords  = {Topic modelling, LDA, Domain-specific corpora},
  rights    = {© 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
}

@Article{Yang2023,
  author        = {Yang, Hui and Yue, Sifu and He, Yunzhong},
  title         = {Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions},
  year          = {2023},
  month         = jun,
  abstract      = {Auto-GPT is an autonomous agent that leverages recent advancements in adapting Large Language Models (LLMs) for decision-making tasks. While there has been a growing interest in Auto-GPT stypled agents, questions remain regarding the effectiveness and flexibility of Auto-GPT in solving real-world decision-making tasks. Its limited capability for real-world engagement and the absence of benchmarks contribute to these uncertainties. In this paper, we present a comprehensive benchmark study of Auto-GPT styled agents in decision-making tasks that simulate real-world scenarios. Our aim is to gain deeper insights into this problem and understand the adaptability of GPT-based agents. We compare the performance of popular LLMs such as GPT-4, GPT-3.5, Claude, and Vicuna in Auto-GPT styled decision-making tasks. Furthermore, we introduce the Additional Opinions algorithm, an easy and effective method that incorporates supervised/imitation-based learners into the Auto-GPT scheme. This approach enables lightweight supervised learning without requiring fine-tuning of the foundational LLMs. We demonstrate through careful baseline comparisons and ablation studies that the Additional Opinions algorithm significantly enhances performance in online decision-making benchmarks, including WebShop and ALFWorld.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2306.02224},
  eprint        = {2306.02224},
  file          = {:Yang2023 - Auto GPT for Online Decision Making_ Benchmarks and Additional Opinions.pdf:PDF;:http\://arxiv.org/pdf/2306.02224v1:PDF},
  keywords      = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  priority      = {prio1},
  publisher     = {arXiv},
}

@Article{Wang2023,
  author        = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
  title         = {A Survey on Large Language Model based Autonomous Agents},
  year          = {2023},
  month         = aug,
  abstract      = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2308.11432},
  eprint        = {2308.11432},
  file          = {:Wang2023 - A Survey on Large Language Model Based Autonomous Agents.pdf:PDF},
  keywords      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
}

@Article{Xi2023,
  author        = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Gui, Tao},
  title         = {The Rise and Potential of Large Language Model Based Agents: A Survey},
  year          = {2023},
  month         = sep,
  abstract      = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2309.07864},
  eprint        = {2309.07864},
  file          = {:Xi2023 - The Rise and Potential of Large Language Model Based Agents_ a Survey.pdf:PDF},
  keywords      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
}

@Article{Ouyang2022,
  author        = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  title         = {Training language models to follow instructions with human feedback},
  year          = {2022},
  month         = mar,
  abstract      = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2203.02155},
  eprint        = {2203.02155},
  file          = {:Ouyang2022 - Training Language Models to Follow Instructions with Human Feedback.pdf:PDF},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  priority      = {prio1},
  publisher     = {arXiv},
}

@InProceedings{Radford2018,
  author   = {Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever},
  title    = {Improving Language Understanding by Generative Pre-Training},
  year     = {2018},
  file     = {:/home/meinsein/dev/masterarbeit/writing/bib/language_understanding_paper.pdf:PDF},
  priority = {prio1},
}

@Article{Vaswani2017,
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  title         = {Attention Is All You Need},
  year          = {2017},
  month         = jun,
  abstract      = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1706.03762},
  eprint        = {1706.03762},
  file          = {:Vaswani2017 - Attention Is All You Need.pdf:PDF},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  priority      = {prio1},
  publisher     = {arXiv},
}

@Article{Devlin2018,
  author        = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year          = {2018},
  month         = oct,
  abstract      = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1810.04805},
  eprint        = {1810.04805},
  file          = {:Devlin2018 - BERT_ Pre Training of Deep Bidirectional Transformers for Language Understanding.pdf:PDF},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  priority      = {prio1},
  publisher     = {arXiv},
}

 
@InProceedings{Bender2023b,
  author    = {Bender, Magnus and Braun, Tanya and Möller, Ralf and Gehrke, Marcel},
  booktitle = {2023 IEEE 17th International Conference on Semantic Computing (ICSC)},
  title     = {Unsupervised Estimation of Subjective Content Descriptions},
  year      = {2023},
  month     = feb,
  publisher = {IEEE},
  doi       = {10.1109/icsc56153.2023.00052},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Private Papers\;0\;1\;0x8a8a8aff\;\;\;;
}
