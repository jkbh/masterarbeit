@inproceedings{Schiff2023,
  AUTHOR    = {Schiff, Simon and Möller, Ralf},
  BOOKTITLE = {Proceedings of the Workshop on Humanities-Centered Artificial Intelligence (CHAI 2023)},
  TITLE     = {Persistent Data, Sustainable Information},
  PAGES     = {5--14},
  PUBLISHER = {CEUR Workshop Proceedings},
  URL       = {https://ceur-ws.org/Vol-3580/paper1.pdf},
  ABSTRACT  = {In almost all academic fields, results are derived from found evidence such as digitized objects stored at a repository. Deriving results from such repositories can be time and cost intensive, as data is often difficult to reuse. Guidelines such as FAIR (Findability, Accessibility, Interoperable, and Reuse) are intended to disseminate the proper archiving of research data such that, among others, archived data is easy to reuse. However, we argue that even if one follows FAIR guidelines, it is still challenging to derive results by reusing data. First of all, before reusing data from any repository, one needs to find the data. Search engines can index data stored at repositories, as data is associated with metadata as proposed by the FAIR guidelines. Deciding whether the data found is relevant usually requires downloading, extracting and visualizing the entire dataset which is time-consuming and costly. We propose that data need to be archived by associating it with metadata that determines how data can be prepared to support decision making. Metadata links data with executable code that can create an information system from associated data on demand. With our solution, data is easier to reuse, as one can decide whether found data is relevant. In addition, if the data is found to be relevant, our information system allows researchers to clearly refer to specific regions in the data for data governance.},
  GROUPS    = {Uni Lübeck},
  KEYWORDS  = {persistent data, sustainable information, research data, data management},
  MONTH     = sep,
  RIGHTS    = {© 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
  YEAR      = {2023}
}

@misc{Wang2023,
  AUTHOR        = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
  TITLE         = {A Survey on Large Language Model based Autonomous Agents},
  DOI           = {10.48550/ARXIV.2308.11432},
  EPRINT        = {2308.11432},
  ABSTRACT      = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
  ARCHIVEPREFIX = {arXiv},
  COPYRIGHT     = {Creative Commons Attribution 4.0 International},
  FILE          = {:Wang2023 - A Survey on Large Language Model Based Autonomous Agents.pdf:PDF},
  GROUPS        = {Information Retrieval, LLMs, Masterarbeit},
  KEYWORDS      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
  MONTH         = aug,
  PRIMARYCLASS  = {cs.AI},
  PUBLISHER     = {arXiv},
  YEAR          = {2023}
}

@misc{Xi2023,
  AUTHOR        = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Gui, Tao},
  TITLE         = {The Rise and Potential of Large Language Model Based Agents: A Survey},
  DOI           = {10.48550/ARXIV.2309.07864},
  EPRINT        = {2309.07864},
  ABSTRACT      = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
  ARCHIVEPREFIX = {arXiv},
  COPYRIGHT     = {arXiv.org perpetual, non-exclusive license},
  FILE          = {:Xi2023 - The Rise and Potential of Large Language Model Based Agents_ a Survey.pdf:PDF},
  GROUPS        = {LLMs, Masterarbeit},
  KEYWORDS      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
  MONTH         = sep,
  PRIMARYCLASS  = {cs.AI},
  PUBLISHER     = {arXiv},
  YEAR          = {2023}
}

@inproceedings{Ouyang2022,
  AUTHOR    = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  TITLE     = {Training language models to follow instructions with human feedback},
  EDITOR    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  PAGES     = {27730--27744},
  PUBLISHER = {Curran Associates, Inc.},
  URL       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
  VOLUME    = {35},
  FILE      = {:Ouyang2022 - Training Language Models to Follow Instructions with Human Feedback.pdf:PDF},
  GROUPS    = {LLMs, Masterarbeit},
  YEAR      = {2022}
}

@misc{Radford2018,
  AUTHOR = {Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever},
  TITLE  = {Improving Language Understanding by Generative Pre-Training},
  URL    = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  FILE   = {:Radford2018 - Improving Language Understanding by Generative Pre Training.pdf:PDF},
  GROUPS = {LLMs, Masterarbeit},
  YEAR   = {2018}
}

@inproceedings{Vaswani2017,
  AUTHOR         = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  BOOKTITLE      = {Advances in Neural Information Processing Systems},
  TITLE          = {Attention is All you Need},
  EDITOR         = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  PUBLISHER      = {Curran Associates, Inc.},
  URL            = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  VOLUME         = {30},
  FILE           = {:Vaswani2017 - Attention Is All You Need.pdf:PDF},
  GROUPS         = {LLMs, Masterarbeit},
  QUALITYASSURED = {qualityAssured},
  YEAR           = {2017}
}

@misc{Devlin2019,
  AUTHOR        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  TITLE         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  EPRINT        = {1810.04805},
  ARCHIVEPREFIX = {arXiv},
  FILE          = {:Devlin2019 - BERT_ Pre Training of Deep Bidirectional Transformers for Language Understanding.pdf:PDF},
  GROUPS        = {Masterarbeit},
  PRIMARYCLASS  = {cs.CL},
  YEAR          = {2019}
}

@inproceedings{Wei2022,
  AUTHOR     = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
  BOOKTITLE  = {Advances in Neural Information Processing Systems},
  DATE       = {2022},
  TITLE      = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  EDITOR     = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  PAGES      = {24824--24837},
  PUBLISHER  = {Curran Associates, Inc.},
  URL        = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
  VOLUME     = {35},
  FILE       = {:NEURIPS2022_9d560961 - Chain of Thought Prompting Elicits Reasoning in Large Language Models.pdf:PDF},
  GROUPS     = {LLMs, Masterarbeit},
  PRIORITY   = {prio2},
  READSTATUS = {skimmed}
}

@inproceedings{Asselborn2023,
  AUTHOR     = {Thomas Asselborn and Sylvia Melzer and Said Aljoumani and Magnus Bender and Florian Andreas Marwitz and Konrad Hirschler and Ralf M\"oller},
  BOOKTITLE  = {Proceedings of the Workshop on Humanities-Centred Artificial Intelligence (CHAI 2023)},
  TITLE      = {Fine-tuning BERT Models on Demand for Information Systems Explained Using Training Data from Pre-modern Arabic},
  PAGES      = {38--51},
  PUBLISHER  = {CEUR Workshop Proceedings},
  URL        = {https://ceur-ws.org/Vol-3580/paper5.pdf},
  ABSTRACT   = {Humanities scholars can use Large Language Models (LLMs) to simplify their work to analyse texts or recognise patterns in data. However, if a domain-specific problem needs to be solved, the existing models need to be fine-tuned. In the humanities, there is less training data available for fine-tuning, but there are increasing information systems with research data in that can be used for this purpose. To use data from information systems, humanities researchers need to be able to transform research data from information systems to training data so that it is suitable for training LLMs. However, assigning the correct labels (e.g., person, location, date) to the data can be challenging. This article describes how to fine-tune BERT models on demand for information systems explained using training data from pre-modern Arabic. The result we have achieved is that all archived research data can be used in a research data repository for fine-tuning models in short time and in a simplified way, i.e., without being an IT expert. Since the data was available in canonical form, it was possible to specify which fields could be assigned to which label by means of a manifest file. The results we obtained show that the fine-tuning process can be done in just a few minutes using a sample dataset and BERT. The Fine-tuning on Demand (FToD) process identified names of people, places, or dates that could not be recognised by the pre-trained model.},
  GROUPS     = {Uni Lübeck},
  KEYWORDS   = {Fine-tuning on demand, BERT, pre-modern Arabic, manifest file Asselborn), 0000-0002-0144-5429 (S. Melzer), 0009-0004-8306-8621 (S. Aljoumani), 0000-0002-1854-225X (M. Bender), 0000-0002-9683-5250 (F. A. Marwitz), 0000-0002-6012-7711 (K. Hirschler), 0000-0002-1174-3323 (R. Möller)},
  MONTH      = sep,
  PRIORITY   = {prio1},
  READSTATUS = {read},
  RELEVANCE  = {relevant},
  RIGHTS     = {© 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
  YEAR       = {2023}
}

@inproceedings{Bender2023a,
  AUTHOR    = {Magnus Bender and Tanya Braun and Ralf M\"oller and Marcel Gehrke},
  BOOKTITLE = {17th {IEEE} International Conference on Semantic Computing, ({ICSC} 2023), February 1-3},
  TITLE     = {Unsupervised Estimation of Subjective Content Descriptions},
  DOI       = {10.1109/icsc56153.2023.00052},
  PUBLISHER = {{IEEE}},
  URL       = {https://www.ifis.uni-luebeck.de/uploads/tx_wapublications/uscd-ieee_preprint_pre-review_public.pdf},
  COMMENT   = {Calculating the SCD-word distributions without an SCD repository.
               Needed for unlabeled corpora of documents.},
  GROUPS    = {Uni Lübeck},
  KEYWORDS  = {Subjective Content Descriptions; Text Mining;Text Annotation;Sentence clustering},
  YEAR      = {2023}
}

@inproceedings{Bender2023b,
  AUTHOR    = {Magnus Bender and Kira Schwandt and Ralf M\"oller and Marcel Gehrke},
  BOOKTITLE = {Proceedings of the Workshop on Humanities-Centred Artificial Intelligence (CHAI 2023)},
  TITLE     = {FrESH – Feedback-reliant Enhancement of Subjective Content Descriptions by Humans},
  PAGES     = {15--24},
  PUBLISHER = {CEUR Workshop Proceedings},
  URL       = {https://ceur-ws.org/Vol-3580/paper3.pdf},
  GROUPS    = {Uni Lübeck},
  MONTH     = sep,
  YEAR      = {2023}
}

@inproceedings{Bender2023,
  AUTHOR    = {Magnus Bender and Tanya Braun and Ralf M\"oller and Marcel Gehrke},
  BOOKTITLE = {KI 2023: Advances in Artificial Intelligence},
  TITLE     = {LESS is More: LEan Computing for Selective Summaries},
  DOI       = {10.1007/978-3-031-42608-7_1},
  PAGES     = {1--14},
  PUBLISHER = {Springer Nature Switzerland},
  GROUPS    = {Uni Lübeck},
  JOURNAL   = {International Journal of Semantic Computing},
  YEAR      = {2023}
}

@inproceedings{Kuhr2019,
  AUTHOR    = {Felix Kuhr and Tanya Braun and Magnus Bender and Ralf M\"oller},
  BOOKTITLE = {Proceedings of {AI} 2019: Advances in Artificial Intelligence},
  TITLE     = {{To Extend or not to Extend? Context-specific Corpus Enrichment}},
  DOI       = {10.1007/978-3-030-35288-2_29},
  ISBN      = {978-3-030-35288-2},
  PAGES     = {357--368},
  PUBLISHER = {Springer},
  SERIES    = {Lecture Notes in Computer Science},
  URL       = {https://www.ifis.uni-luebeck.de/uploads/tx_wapublications/public_AI2019_paper_79.pdf},
  VOLUME    = {11919},
  GROUPS    = {Uni Lübeck},
  YEAR      = {2019}
}

@misc{Yang2023,
  AUTHOR        = {Hui Yang and Sifu Yue and Yunzhong He},
  TITLE         = {Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions},
  DOI           = {10.48550/ARXIV.2306.02224},
  EPRINT        = {2306.02224},
  ABSTRACT      = {Auto-GPT is an autonomous agent that leverages recent advancements in adapting Large Language Models (LLMs) for decision-making tasks. While there has been a growing interest in Auto-GPT stypled agents, questions remain regarding the effectiveness and flexibility of Auto-GPT in solving real-world decision-making tasks. Its limited capability for real-world engagement and the absence of benchmarks contribute to these uncertainties. In this paper, we present a comprehensive benchmark study of Auto-GPT styled agents in decision-making tasks that simulate real-world scenarios. Our aim is to gain deeper insights into this problem and understand the adaptability of GPT-based agents. We compare the performance of popular LLMs such as GPT-4, GPT-3.5, Claude, and Vicuna in Auto-GPT styled decision-making tasks. Furthermore, we introduce the Additional Opinions algorithm, an easy and effective method that incorporates supervised/imitation-based learners into the Auto-GPT scheme. This approach enables lightweight supervised learning without requiring fine-tuning of the foundational LLMs. We demonstrate through careful baseline comparisons and ablation studies that the Additional Opinions algorithm significantly enhances performance in online decision-making benchmarks, including WebShop and ALFWorld.},
  ARCHIVEPREFIX = {arXiv},
  COPYRIGHT     = {Creative Commons Attribution 4.0 International},
  FILE          = {:Yang2023 - Auto GPT for Online Decision Making_ Benchmarks and Additional Opinions.pdf:PDF},
  GROUPS        = {LLMs, Masterarbeit},
  KEYWORDS      = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  MONTH         = jun,
  PRIMARYCLASS  = {cs.AI},
  PUBLISHER     = {arXiv},
  RELEVANCE     = {relevant},
  YEAR          = {2023}
}

@inproceedings{Redzuan2023,
  AUTHOR    = {Nadja Redzuan and Marcel Gehrke and Ralf M\"oller and Tanya Braun},
  BOOKTITLE = {Proceedings of the Workshop on Humanities-Centred Artificial Intelligence (CHAI 2023)},
  TITLE     = {On Domain-specific Topic Modelling Using the Case of a Humanities Journal},
  PAGES     = {25--37},
  PUBLISHER = {CEUR Workshop Proceedings},
  URL       = {https://ceur-ws.org/Vol-3580/paper4.pdf},
  GROUPS    = {Uni Lübeck},
  MONTH     = sep,
  YEAR      = {2023}
}

@inproceedings{Lewis2020,
  AUTHOR    = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  TITLE     = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  EDITOR    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  PAGES     = {9459--9474},
  PUBLISHER = {Curran Associates, Inc.},
  URL       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
  VOLUME    = {33},
  FILE      = {:Lewis2020 - Retrieval Augmented Generation for Knowledge Intensive NLP Tasks.pdf:PDF},
  GROUPS    = {Masterarbeit},
  YEAR      = {2020}
}

@misc{Schiff2020,
  AUTHOR = {Simon Schiff},
  DATE   = {2020},
  TITLE  = {AI-based Companion Services for Humanities}
}


@inproceedings{Park2023,
  AUTHOR     = {Park, Joon Sung and O’Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  BOOKTITLE  = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  DATE       = {2023-10},
  TITLE      = {Generative Agents: Interactive Simulacra of Human Behavior},
  DOI        = {10.1145/3586183.3606763},
  PUBLISHER  = {ACM},
  SERIES     = {UIST ’23},
  URL        = {https://dl.acm.org/doi/pdf/10.1145/3586183.3606763},
  COLLECTION = {UIST ’23},
  COMMENT    = {Research chatbot has to be helpful and not generate human behavour.
                
                What might be intersting is the modeling of the state of other agents, which could model the user that prompts the chatbot.},
  FILE       = {:Park2023 - Generative Agents_ Interactive Simulacra of Human Behavior.pdf:PDF},
  PRIORITY   = {prio3},
  READSTATUS = {skimmed}
}

@misc{Liu2023,
  AUTHOR      = {Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
  DATE        = {2023},
  TITLE       = {Lost in the Middle: How Language Models Use Long Contexts},
  EPRINT      = {2307.03172},
  EPRINTCLASS = {cs.CL},
  EPRINTTYPE  = {arXiv},
  FILE        = {:Liu2023 - Lost in the Middle_ How Language Models Use Long Contexts.pdf:PDF},
  GROUPS      = {Masterarbeit}
}

@article{Mialon2023,
  AUTHOR      = {Mialon, Grégoire and Fourrier, Clémentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  DATE        = {2023-11-21},
  TITLE       = {GAIA: a benchmark for General AI Assistants},
  DOI         = {10.48550/ARXIV.2311.12983},
  EPRINT      = {2311.12983},
  EPRINTCLASS = {cs.CL},
  EPRINTTYPE  = {arXiv},
  ABSTRACT    = {We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92\% vs. 15\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available at https://huggingface.co/gaia-benchmark.},
  COPYRIGHT   = {arXiv.org perpetual, non-exclusive license},
  FILE        = {:Mialon2023 - GAIA_ a Benchmark for General AI Assistants.pdf:PDF},
  KEYWORDS    = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  PUBLISHER   = {arXiv},
  YEAR        = {2023}
}

@article{Gao2023,
  AUTHOR      = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Guo, Qianyu and Wang, Meng and Wang, Haofen},
  DATE        = {2023-12-18},
  TITLE       = {Retrieval-Augmented Generation for Large Language Models: A Survey},
  DOI         = {10.48550/ARXIV.2312.10997},
  EPRINT      = {2312.10997},
  EPRINTCLASS = {cs.CL},
  EPRINTTYPE  = {arXiv},
  ABSTRACT    = {Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces the metrics and benchmarks for assessing RAG models, along with the most up-to-date evaluation framework. In conclusion, the paper delineates prospective avenues for research, including the identification of challenges, the expansion of multi-modalities, and the progression of the RAG infrastructure and its ecosystem.},
  COPYRIGHT   = {arXiv.org perpetual, non-exclusive license},
  FILE        = {:Gao2023 - Retrieval Augmented Generation for Large Language Models_ a Survey.pdf:PDF},
  GROUPS      = {LLMs, Masterarbeit},
  KEYWORDS    = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  PUBLISHER   = {arXiv},
  YEAR        = {2023}
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Uni Lübeck\;0\;1\;0x334db3ff\;\;\;;
1 StaticGroup:Information Retrieval\;0\;1\;\;\;\;;
1 StaticGroup:LLMs\;0\;1\;\;\;\;;
1 StaticGroup:Masterarbeit\;0\;1\;0xb39933ff\;\;Nur Paper die wirklich relevant sind\;;
}
