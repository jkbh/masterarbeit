\documentclass[english, version-2022-01]{uzl-thesis}
\usepackage[outputdir=out]{minted}
\UzLStyle{computer modern oldschool design}
\UzLThesisSetup{
Masterarbeit,
Verfasst = {am}{Institut für Informationssysteme},
Titel auf Deutsch = {Nutzung von AutoGPT für Informations-Recherche Agenten},
Titel auf Englisch = {Using AutoGPT for Information Retrieval Agents},
Autor = {Jakob Horbank},
Betreuerin = {Prof. Dr. Ralf Möller},
Studiengang = {Informatik},
Datum = {15. April 2024},
Abstract = {It is about saying ``hello'' to the world.},
Zusammenfassung = {Es geht darum, der Welt »Hallo« zu sagen.},
Numerische Bibliographie
}
\addbibresource{bib/masterarbeit.bib}

\begin{document}
\chapter{Introduction}

This an introduction like not other. Information retrieval hard and stuff. Would be nice to have a chatbot that answer natural language questions and gives sources from research database and even web.

\section{Contributions of this Thesis}

Hopefully the above.

\section{Related Work}

There are different attempts at creating LLM outputs with sources. Perplexity AI hosts a question answering service, that gives source from websites.

\subsection{Open Source LLM Agents}
\begin{itemize}
	\item AutoGPT
	\item babyagi
\end{itemize}
\subsection{Information Retrieval Applications}

In a variety of application contexts, the answers of an assistent have to be correct. This is espciacally true in reasearch contexts. A model that hallucinates isn't feasable in this case. But while hallucination can be reducing with fine-tuning, it can not be competely eliminated. Perplexity AI is an online service that leverages a language model to provide a search that generates an answer from different sources in the internet. The content of the answer is then linked to the found sources, so the user can see and verify the result.

As this is a propierty closed source product accessible through a web interface, this is not a useful to create our own research assistant. The provided API simply hosts different LLMs and allows for prompting them. There is no possibilty of of fine-tuning.

\section{Structure of this Thesis}
I do this then that and then that.

\chapter{Backgrounds}

Different branches of research were used to build upon in this work. These topics and how their are connected to my work are explained in more detail in this section.
\section{Information Retrieval}

Handling large databases filled with information of different is a common problem. There has been extensive reasearch on database architectures and indexing algorithms.

\section{Agents}

Although the notion of agent is not new, it recently gained attention in combination with the rise of generative language models.
\section{Large Language Models}

Natural language processing is a long studied research area of computer science. In recent years delevopments have significantly sped up, with the introduction of deep learning into NLP. The transformer archticture has been the basis for all advancements in recent years.

\subsection{Transformer}
Until 2017, the dominating strategy to train models for language tasks revolved around recurrent structures. Every sentence token was represented as a hidden state that resultet from all the previous tokens. While this approach has a reasonable motivation, the sequential nature contraints computation speed. There is no way to compute the recurrent architectures in parallel.

The transformer model \cite{NIPS2017_3f5ee243} solely relys on these attention mechanisms. In particular, they employ self-attention and multi-headed self-attention layers

Attention mechanism allow learning dependencies between tokens in sentences without regard to their distance. Their non-sequential nature allow for massive parallelization.

Self-Attention is an attention mechanism that computes relations between different positions of the same sequence with goal of finding a representation of the sequence.

Like a typical sequence modeling architecture, a transformer consists of an enconder and a decoder. The encoder has two parts that are stacked on each other multiple times. The first part is a mulit-head self-attention block, the second part is a classic fully connected feed foward block.

\subsection{BERT}

The BERT model family consists of encoder only transfomers. They are trained to generating rich embeddengs of tokens for use in a range of different downstream tasks.

\subsection{GPT}

GPT models are decoder only transformer models. They excel at text generation tasks. they are trained to predict the next token of a sequence.
\subsection{Instruction tuned Language Models}

Language models trained to predict the next token of a sequence. While alot of knowledge is captured in the weights of the model, most information in the internet is not formatted in conversatonal style. Because of this, langauge models need to be prompted in a specific way to be effective. The prompt has to be written in a way that the continuation of it yields the desired output. This is not optimal for human users, as they would rather write in a conversational style. Instruction tuned models solve that problem.

Instruct models are fine-tuned version of language models. Capturing the intent of the user is a key challenge for language models. This process is called \textit{alignment}. popular approach to the alignment problem is reinforcement learning with human feedback (RLHF). Handcrafted prompts are used to fine-tune GPT-3. The outputs of the model are collected into a set and ranked by humans. This set is then used to train a reward model. With this reward model, the language model is further fine-tuned. The resulting model is called \textit{InstructGPT} and performs better than the baseline GPT-3 model.

Large language models are trained to predict the next token of a sequence, not to follow the instruction of the user. This leads to some unwanted results, such as toxic, harmful answers or fabricated information that is not true.

\subsection{LLM Agents}

AutoGPT is an open source project that tries to leverage LLMs to function as an agent controller. GPT models are extended with different modules to creat an agent that receives a goal and then acts towards reaching it. To reach the goal AutoGPT plans, has a memory, and reflects on past actions.

AutoGPT is an example implementation of an agent. The project tries to become a framework to build agents with different architectures.
AutoGPT employs the key components of an agent outlined in \cite{Wang2023}. The profile module allows the agent to assume different roles, such as expert, teacher or coder. Profile are LLM specific as each model responds best for different prompting styles. Memory is implemented through a database.

Other open source llm agent systems
\begin{itemize}
	\item miniagi \href{}{miniagi}
	\item babyagi \href{https://github.com/yoheinakajima/babyagi}{miniagi}
\end{itemize}

\chapter{Analysis of AutoGPT for Information Retrieval Tasks}


AutoGPT is an open source project that tries to 'make GPT fully autonomous'. GPT langauge models are used to control an agent that works towards reaching a stated goal. The project contains a core general purpose agent with a predefined set of abilities. Addidtionaly a baseline sdk is beeing developed to build custom agents.
\section{Agent Theory Backgrounds}

\section{Architecture Overview}

The AutoGPT agent is modeled after the classic agent architecture. After a goal given to the agent, it creates a task that has to be completed. Then it completes steps that make up that task. In each step an action is run.

AutoGPT is divided into four modules. The \textit{brain} is the main module that controls the agent. In AutoGPT this is realized by prompting the language model in a structured way. Using the chat system prompt, the language model is prompted to answer a structred format. The format is shown in listing \ref{listing:autogpt_system_prompt}. Different techniques are implemented in this structure. The language model is forced not only to plan the next step, but also to explain the choice for the chosen step and to add self-criticism. An extra output for the human user is also returned. The second part of the answer is the actual next action with the needed arguments. The action make up the second module of the agent. In this module the abilities of the agent are defined. These can be file operations, database queries or web search functionalities. The third module is the \textit{memory}. Memories are modeled after humans which have short and long-term memory. Short term memory can be implemented as an in-memory list messages to the language model. Long-term memory needs a persistent store such as a database. A popular option for language models are vector databases that work with embeddings.

\begin{listing}
	\inputminted{jinja}{include/code/system-format.j2}
	\caption{AutoGPT System Prompt}
	\label{listing:autogpt_system_prompt}
\end{listing}

Currently, AutoGPT is only compatible with OpenAI models. Switching to a different model is not that hard, because only the API format would have to be changed. The challenge is to switch between different prompting styles, as every model needs to be prompted differently. For example OpenAI models benefit from profile sentences like "You are an expert in computer science", while Anthropics Claude does not\dots

\section{Default Abilities}

\section{Information Retrieval Capabilities}

The AutoGPT Agent has different abilities that can be uitilzed for information retrieval. Notably, it is able to search the web and operate on the file system.

The web search is implemented by a two step process. First a search API like duckduckgo is called to get a list of relevant pages. Then the page contents are scraped with a headless browser. It is possible to read and write to text files. Other document types are processed by basic text extraction tools to get the plain text.

For longer files such as scientific journals the extracted fulltext is too long for the language model. The AutoGPT agent has no ability to chunk the text into smaller chunk or store it in a database. This is a limitation that needs to be addressed for information retrieval tasks over a research database repository.

Having a vectorestore would enable techniques such as retrieval augmented generation. The agent would get a prompt which a question over the RDR and choose an action to start a semantic search over the vectorstore. The result of the search are the chunks that are semantically closest to the question. These chunks can then be inclued as context for the LLM prompt to generate an answer.

The default agent has a tendency towards searching the web for information. We want an agent that prioritizes information that is present in the research repository. This needs to be addressed in the prompting techniques of the agent.

\chapter{Retrieval Augmentated Generation Agent}

Large language models are excellent at generating text in a variety of styles. Different approaches try to embed information sources into the generated text. One appreach is to fine-tune the language model on a dataset that contains the information. This can work, but often there is not enough data to shift the model weights enough. A more promising approach is retrieval augmented generation. Before generating an answer to a prompt, a vectorestore is searched for relevant information. The retrieved information is then included in the prompt.
\section{Retrieval Augmented Generation}

\begin{itemize}
	\item what is a vectorestore compared to other databases
	\item how are documents stored and queried
	\item how is the information included in the prompt
\end{itemize}


\section{Extending AutoGPT with retrieval capailities}

\begin{itemize}
	\item Custom agent Loop
	\item Custom abilities
	\item Further guidances for the agent
\end{itemize}

\chapter{Creating IR Agent Benchmarking Challenges}

\section{How the benchmarking system works}

\begin{itemize}
	\item Level based system
	\item Dependencies
\end{itemize}
\section{Existing Benchmarks}

\begin{itemize}
	\item What exists
	\item what is missing for local IR 
\end{itemize}

\section{Custom Benchmarks for local IR over journals}

\begin{itemize}
	\item How i benchmark the steps in IR
\end{itemize}
\chapter{Results}

% \chapter{Handcrafted Prompts as a Research Chatbot}

% \begin{itemize}
% 	\item First, only crafting prompt for default GPT-3.5
% 	\item What could be questions about hadith corpus?
% 	      \begin{itemize}
% 		      \item Ask for classifaction outputs

% 		            These would be the same as the BERT classifactions, just as question sentences. Questions about contained persons, locations, dates, \dots.
% 		      \item Further questions

% 		            Information probably not contained in corpus. LLM might be able to give some reasoning learned in pre-training.
% 	      \end{itemize}
% 	\item How to design prompts?

% 	      Few-shot with examples or Zero Shot? Include example labelings from corpus.
% 	\item How to include corpus?

% 	      The corpus can be large. Large than the context length of LLMs. So first try handling a single document
% 	\item How to craft testing examples?

% 	      For the labeling questions create examples for amended data.
% \end{itemize}


% \chapter{Custom Fine-Tuned Language Model as a Research Chatbot}
% \begin{itemize}
% 	\item Here, explore fine-tuning a LLM
% 	\item Identify problems of non fine-tuned Chatbot
% 	\item How to create fine-tuned dataset

% 	      Again, for labeling questions from amended data, but with focus on weaknesses.
% \end{itemize}

\chapter{Conclusion}
Saying hello world is quite easy.
\begin{bibtex-entries}
@TechReport{Kernighan1974,
	author = {Brian Kernighan},
	title = {Programming in C – A Tutorial},
	institution = {Bell Laboratories},
	year = {1974}
}
\end{bibtex-entries}
\end{document}
