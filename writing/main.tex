\documentclass[english, version-2022-01]{uzl-thesis}
\UzLStyle{computer modern oldschool design}
\UzLThesisSetup{
Masterarbeit,
Verfasst = {am}{Institut für Informationssysteme},
Titel auf Deutsch = {Hallo Welt},
Titel auf Englisch = {Hello World},
Autor = {Jakob Horbank},
Betreuerin = {Prof. Dr. Ralf Möller},
Studiengang = {Informatik},
Datum = {15. April 2024},
Abstract = {It is about saying ``hello'' to the world.},
Zusammenfassung = {Es geht darum, der Welt »Hallo« zu sagen.},
Numerische Bibliographie
}
\begin{document}
\chapter{Introduction}
\section{Contributions of this Thesis}
This thesis says ``Hello World!'', see also \cite{Kernighan1974}.
\section{Related Work}
There are many hello world programs.

\subsection{Transformer}
Transformer Netzwerke sind supertoll. Encoder-Decoder architecture gained traction as they allowed for the computation of better intermedate representation of the data.

Until 2017, the dominating strategy to train models for language tasks revolved around recurrent structures. Every sentence token was represented as a hidden state that resultet from all the previous tokens. While this approach has a reasonable motivation, the sequential nature contraints computation speed. There is no way to compute the recurrent architectures in parallel.

The transformer model propsed by (source) solely relys on these attention mechanisms. In particular, they employ self-attention and multi-headed self-attention layers

Attention mechanism allow learning dependencies between tokens in sentences without regard to their distance. Their non-sequential nature allow for massive parallelization.

Self-Attention is an attention mechanism that computes relations between different positions of the same sequence with goal of finding a representation of the sequence.

Like a typical sequence modeling architecture, a transformer consists of an enconder and a decoder. The encoder has two parts that are stacked on each other multiple times. The first part is a mulit-head self-attention block, the second part is a classic fully connected feed foward block.


\subsection{BERT}
Bert is the best friend of Ernie.

\subsection{Generative Pre-Trained Transformer}
I like GPT because ChatGPT is cool lets goooo.

\subsection{InstructGPT}
With InstructGPT OpenAI(lol) added some magic to the whole thingy. They gave people in nigeria below minimum wage and asked them to rate GPT outputs and trained a reward model that can fine-tune GPT according to human needs. They call this Reinforcement Learning with Human Feedback (RLHF).

Aligning language models is a central research area in creating intelligent systems that humans want to use. As baseline LLMs are training on huge datasets containing all kinds of text documents, the outputs are not necessarily tailored for the questioners needs.
Rather, they input sequence is simply continued with the most likily next tokens. InstructGPT are a family of LLMs created by OpenAI that are fine-tuned with human feedback. With this procedure the models are better at capturing the intent of the user. Furthermore there are improvements in hallucinations and false information.
\subsection{Information Retrieval Agents}
Information is cool and so is retrieving it. But that is hard, because most information is useless. As an example, watch any markus lanz episode.
\section{Structure of this Thesis}
I do this then that and then that.

In Chapter~\vref{chapter-main}, we say hello.
\chapter{Handcrafted Prompts as a Research Chatbot}
\label{chapter-main}
\begin{itemize}
	\item First, only crafting prompt for default GPT-3.5
	\item What could be questions about hadith corpus?
	      \begin{itemize}
		      \item Ask for classifaction outputs

		            These would be the same as the BERT classifactions, just as question sentences. Questions about contained persons, locations, dates, \dots.
		      \item Further questions

		            Information probably not contained in corpus. LLM might be able to give some reasoning learned in pre-training.
	      \end{itemize}
	\item How to design prompts?

	      Few-shot with examples or Zero Shot? Include example labelings from corpus.
	\item How to include corpus?

	      The corpus can be large. Large than the context length of LLMs. So first try handling a single document
	\item How to craft testing examples?

	      For the labeling questions create examples for amended data.
\end{itemize}
\chapter{Custom Fine-Tuned Language Model as a Research Chatbot}
\begin{itemize}
	\item Here, explore fine-tuning a LLM
	\item Identify problems of non fine-tuned Chatbot
	\item How to create fine-tuned dataset

	      Again, for labeling questions from amended data, but with focus on weaknesses.
\end{itemize}
\chapter{Conclusion}
Saying hello world is quite easy.
\begin{bibtex-entries}
@TechReport{Kernighan1974,
	author = {Brian Kernighan},
	title = {Programming in C – A Tutorial},
	institution = {Bell Laboratories},
	year = {1974}
}
\end{bibtex-entries}
\end{document}
